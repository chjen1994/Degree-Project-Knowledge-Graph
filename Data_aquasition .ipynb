{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os.path\n",
    "my_path = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning approach:\n",
    "\n",
    "#First identify the table of content by regex to get mention of \"Item\" in <Table> <tbody> tag(perhaps extract the table of content)\n",
    "#or search <a href=\"#.....\" util the mention of Item16, signautre\n",
    "#extract all the href in <a> attribute in the table of content (ex. href1, href2, href3 …)\n",
    "\n",
    "#Second have algorithm to identify <a name=\"href1\"> and save the content between href1 and href2, href2 and href3, …\n",
    "\n",
    "#Third, clean the html tags in the contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html(path):\n",
    "    return BeautifulSoup(open(path,\"r\"), 'html.parser')\n",
    "\n",
    "def soup_get_text(soup):\n",
    "    print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First extract the location of the table of content \n",
    "# use regex to identify Signature/Item 16 and get everything above\n",
    "#only works with the text and items in table of content contains href#if only in the page number, doesn't work\n",
    "\n",
    "def remove_duplicates_in_list(List):\n",
    "    return list(dict.fromkeys(List))\n",
    "def get_table_of_content_links1(soup):\n",
    "    tag = soup.find('a',string=re.compile(\"Financial Statement Schedules\"))\n",
    "    tag_item = soup.find('a',string=re.compile(\"Item 15\"))\n",
    "    if tag==None and tag_item!=None:\n",
    "        tag = tag_item\n",
    "    \n",
    "    links = list()\n",
    "    links.append(tag.get('href')[1:])\n",
    "    for elements in tag.previous_elements:\n",
    "        if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "            #print(elements)\n",
    "            links.append(elements.get('href')[1:])\n",
    "    \n",
    "    links.reverse()\n",
    "    return remove_duplicates_in_list(links)\n",
    "    #print_list(links)\n",
    "\n",
    "#\n",
    "def get_table_of_content_links2(soup):\n",
    "    links = list()\n",
    "    for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "        if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "            links.append(link.get('href')[1:])\n",
    "    return links\n",
    "# or identify the signature link and use find_previous method to extract:extract()?\n",
    "\n",
    "def print_list(lists):\n",
    "    for list in lists:\n",
    "        print(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unable to format tables\n",
    "def get_all_sections(sections, links):\n",
    "    for i in range(len(links)):\n",
    "        if i<len(links)-1:\n",
    "            sections.append(get_section(links[i],links[i+1]))\n",
    "        else:\n",
    "            sections.append(get_last_section(links[i]))\n",
    "\n",
    "def get_section(link1, link2):#need to add \"id\"\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if elements.name=='a' and elements.has_attr('name') and elements['name']==link2:\n",
    "                    break \n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "        elif link.has_attr('id') and link['id']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if elements.name=='a' and elements.has_attr('id') and elements['id']==link2:\n",
    "                    break \n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str\n",
    "\n",
    "def get_last_section(link1):\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "        elif link.has_attr('id') and link['id']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reader(path):\n",
    "    files = []\n",
    "    for r,d,f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r,file))\n",
    "                \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the path of 10k file and store in list name files\n",
    "files = [];\n",
    "files = file_reader(my_path+\"/sec_edgar_filings\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the file to bs4\n",
    "#you may select which file to read by changing the index in the files list\n",
    "\n",
    "soup=load_html(files[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_table_of_content_links1(soup)\n",
    "print_list(links)\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections=list()\n",
    "get_all_sections(sections,links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sections[0])\n",
    "text_file = open(\"sample_KG.txt\", \"w\")\n",
    "text_file.write(sections[0])\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MSFT_1_path='sec_edgar_filings/MSFT/10-K/0001193125-15-272806.txt'\n",
    "MSFT_2_path=\"sec_edgar_filings/MSFT/10-K/0001564590-18-019062.txt\"\n",
    "APPL_1_path='sec_edgar_filings/AAPL/10-K/0000320193-19-000119.txt'\n",
    "APPL_2_path='sec_edgar_filings/AAPL/10-K/0000320193-18-000145.txt'\n",
    "VISA_1_path='sec_edgar_filings/V/10-K/0001403161-18-000055.txt'\n",
    "VISA_2_path='sec_edgar_filings/V/10-K/0001403161-19-000050.txt'\n",
    "GOOG_1_path='sec_edgar_filings/GOOG/10-K/0001652044-19-000004.txt'\n",
    "GOOG_2_path='sec_edgar_filings/GOOG/10-K/0001652044-20-000008.txt'\n",
    "BLK_1_path='sec_edgar_filings/BLK/10-K/0001564590-19-005479.txt'\n",
    "BLK_2_path='sec_edgar_filings/BLK/10-K/0001564590-20-007807.txt'\n",
    "BA_1_path='sec_edgar_filings/BA/10-K/0000012927-19-000010.txt'\n",
    "BA_2_path='sec_edgar_filings/BA/10-K/0000012927-20-000014.txt'\n",
    "FORD_1_path='sec_edgar_filings/F/10-K/0000037996-20-000010.txt'\n",
    "FORD_2_path='sec_edgar_filings/F/10-K/0000037996-19-000012.txt'\n",
    "\n",
    "soup=load_html(MSFT_1_path)\n",
    "\"\"\"\n",
    "#soup_get_text(soup)\n",
    "#visa, msft, goog, blk work\n",
    "#, ba_1_path uses <a id=\"links\">\n",
    "#ford dont work cuz href is at the page number in toc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#get table of content\n",
    "\n",
    "\"identify the mention of Signatures\"\n",
    "tag = soup.find('a',string=re.compile(\"Signatures\"))\n",
    "#print(tag)\n",
    "links = list()\n",
    "for elements in tag.previous_elements:\n",
    "    if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "        #print(elements)\n",
    "        links.append(elements.get('href')[1:])\n",
    "        \n",
    "links.reverse()\n",
    "#print_list(links)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the html\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(open(\"/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-19-027952.txt\"), 'html.parser')\n",
    "#print(soup.prettify())\n",
    "print(soup.get_text())\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#get all href from the table of content \n",
    "Links = list()\n",
    "for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "    if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "        Links.append(link.get('href')[1:])\n",
    "for link in Links:\n",
    "    print(link)\n",
    "        \n",
    "    #remove the '#' and store them in a list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get section \n",
    "#example get section between Item 1 and Item 1A\n",
    "\n",
    "#Problem also get the sentences\n",
    "\"\"\"\n",
    "Sections=\"\"\n",
    "for link in soup.find_all('a'):\n",
    "    if link.has_attr('name') and link['name']=='ITEM_1_BUSINESS':\n",
    "        for elements in link.next_elements:\n",
    "            if elements.name=='a' and elements.has_attr('name') and elements['name']=='ITEM_1B_UNRESOLVED_STAFF_COMMENTS':\n",
    "                break\n",
    "                \n",
    "            \n",
    "            if str(elements)[0] != '<':\n",
    "                Sections = Sections + str(elements)\n",
    "                print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            \n",
    "        break\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
