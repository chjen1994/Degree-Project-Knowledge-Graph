{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re\n",
    "import os.path\n",
    "my_path = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning approach:\n",
    "\n",
    "#First identify the table of content by regex to get mention of \"Item\" in <Table> <tbody> tag(perhaps extract the table of content)\n",
    "#or search <a href=\"#.....\" util the mention of Item16, signautre\n",
    "#extract all the href in <a> attribute in the table of content (ex. href1, href2, href3 …)\n",
    "\n",
    "#Second have algorithm to identify <a name=\"href1\"> and save the content between href1 and href2, href2 and href3, …\n",
    "\n",
    "#Third, clean the html tags in the contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html(path):\n",
    "    return BeautifulSoup(open(path,\"r\"), 'html.parser')\n",
    "def load_xml(path):\n",
    "    return BeautifulSoup(open(path,\"r\"), 'lxml')\n",
    "\n",
    "#get the text from soup\n",
    "def soup_get_text(soup):\n",
    "    print(soup.get_text())\n",
    "\n",
    "#write \"string\" to file from \"path\"\n",
    "def write_file(path, string):\n",
    "    text_file = open(path, \"w\")\n",
    "    text_file.write(string)\n",
    "    text_file.close()\n",
    "\n",
    "\n",
    "#print elements from list\n",
    "def print_list(lists):\n",
    "    for list in lists:\n",
    "        print(list)\n",
    "\n",
    "#read all the text file in the directory\n",
    "def file_reader(path):\n",
    "    files = []\n",
    "    for r,d,f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r,file))\n",
    "                \n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First extract the location of the table of content by\n",
    "#using regex to identify \"Item 15\" or \"Financial Statement Schedules\"\n",
    "#get every links <a href=\"#\"> above \"Item 15\" or \"Financial Statement Schedules\"\n",
    "#only works with the text and items in table of content contains href#\n",
    "#if only in the page number, doesn't work\n",
    "def get_table_of_content_links(soup):\n",
    "    tag = soup.find('a',string=re.compile(\"Financial Statement Schedules\"))\n",
    "    tag_item = soup.find('a',string=re.compile(\"Item 15\"))\n",
    "    if tag==None and tag_item!=None:\n",
    "        tag = tag_item\n",
    "    \n",
    "    links = list()\n",
    "    links.append(tag.get('href')[1:])\n",
    "    for elements in tag.previous_elements:\n",
    "        if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "            #print(elements)\n",
    "            links.append(elements.get('href')[1:])\n",
    "    \n",
    "    links.reverse()\n",
    "    print(\"total section: \",len(remove_duplicates_in_list(links)))\n",
    "    return remove_duplicates_in_list(links)\n",
    "\n",
    "#remove dupicated items\n",
    "def remove_duplicates_in_list(List):\n",
    "    return list(dict.fromkeys(List))\n",
    "\"\"\"\n",
    "def get_table_of_content_links2(soup):\n",
    "    links = list()\n",
    "    for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "        if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "            links.append(link.get('href')[1:])\n",
    "    return links\n",
    "# or identify the signature link and use find_previous method to extract:extract()?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get every sections by using the links to identify <a>\n",
    "def get_all_sections(links, soup):\n",
    "    sections=list()\n",
    "    for i in range(len(links)):\n",
    "        if i<len(links)-1:\n",
    "            sections.append(get_section(links[i],links[i+1],soup))\n",
    "        else:\n",
    "            sections.append(get_last_section(links[i],soup))\n",
    "    return sections\n",
    "\n",
    "#get sections between links[i] and links[i+1]\n",
    "def get_section(link1, link2, soup):\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if elements.name=='a' and elements.has_attr('name') and elements['name']==link2:\n",
    "                    break \n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "        elif link.has_attr('id') and link['id']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if elements.name=='a' and elements.has_attr('id') and elements['id']==link2:\n",
    "                    break \n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str\n",
    "\n",
    "#get last section that is between <a name=\"links[n]\"> to the end of the document\n",
    "def get_last_section(link1,soup):\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "        elif link.has_attr('id') and link['id']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the tag that is not <type>10-k using xml\n",
    "def clean_10k_by_xml(path):\n",
    "    soup=load_xml(path)\n",
    "    extract_type_10k(soup)\n",
    "    write_file(path, str(soup))\n",
    "\n",
    "#remove all tag that is not <type>10-k\n",
    "def extract_type_10k(soup):\n",
    "    tag_list=soup.find_all('type')\n",
    "    for tag in tag_list:\n",
    "        print(tag.next_element)\n",
    "        if '10-K' not in tag.next_element:\n",
    "            tag.extract()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First remove all the tag that is not <type>10-k including xml\n",
    "#Secont extract the links of the items from the table of content \n",
    "#Third get all the sections and store it in a list\n",
    "def get_10k_sections(path):\n",
    "    soup=load_html(path)\n",
    "    sections=get_all_sections(get_table_of_content_links(load_html(path)),soup)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the path of 10k file and store in list name files\n",
    "def get_10k_files():\n",
    "    files = [];\n",
    "    files = file_reader(my_path+\"/sec_edgar_filings\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files=list()\n",
    "files=get_10k_files()\n",
    "print_list(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    clean_10k_by_xml(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections=list()\n",
    "sections=get_10k_sections(files[29])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sections[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLK file 20 link in <p id=\" \"\n",
    "#visa file 2015 have link from item 1-14 but probably forgot to add link to < a item 15 or < a Financial Statement Schedules\n",
    "#visa file 2015 add link to the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all xml tag and save it \n",
    "\"\"\"\n",
    "remove_xml(soup1)\n",
    "text_file = open(\"sec_edgar_filings/clean_data/sample_10k.txt\", \"w\")\n",
    "text_file.write(str(soup1))\n",
    "text_file.close()\n",
    "\"\"\"\n",
    "#load html soup and prettify\n",
    "\"\"\"\n",
    "soup=load_html(\"sec_edgar_filings/clean_data/sample_10k.txt\")\n",
    "soup.prettify()\n",
    "\"\"\"\n",
    "#get table of content links\n",
    "\"\"\"\n",
    "links = get_table_of_content_links1(soup)\n",
    "print_list(links)\n",
    "print(len(links))\n",
    "\"\"\"\n",
    "#get all sections in 10k\n",
    "\"\"\"\n",
    "sections=list()\n",
    "get_all_sections(sections,links)\n",
    "print(sections[21])\n",
    "\"\"\"\n",
    "#write sections to a txt file\n",
    "\"\"\"\n",
    "text_file = open(\"sample_KG.txt\", \"w\")\n",
    "text_file.write(sections[2])\n",
    "text_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MSFT_1_path='sec_edgar_filings/MSFT/10-K/0001193125-15-272806.txt'\n",
    "MSFT_2_path=\"sec_edgar_filings/MSFT/10-K/0001564590-18-019062.txt\"\n",
    "APPL_1_path='sec_edgar_filings/AAPL/10-K/0000320193-19-000119.txt'\n",
    "APPL_2_path='sec_edgar_filings/AAPL/10-K/0000320193-18-000145.txt'\n",
    "VISA_1_path='sec_edgar_filings/V/10-K/0001403161-18-000055.txt'\n",
    "VISA_2_path='sec_edgar_filings/V/10-K/0001403161-19-000050.txt'\n",
    "GOOG_1_path='sec_edgar_filings/GOOG/10-K/0001652044-19-000004.txt'\n",
    "GOOG_2_path='sec_edgar_filings/GOOG/10-K/0001652044-20-000008.txt'\n",
    "BLK_1_path='sec_edgar_filings/BLK/10-K/0001564590-19-005479.txt'\n",
    "BLK_2_path='sec_edgar_filings/BLK/10-K/0001564590-20-007807.txt'\n",
    "BA_1_path='sec_edgar_filings/BA/10-K/0000012927-19-000010.txt'\n",
    "BA_2_path='sec_edgar_filings/BA/10-K/0000012927-20-000014.txt'\n",
    "FORD_1_path='sec_edgar_filings/F/10-K/0000037996-20-000010.txt'\n",
    "FORD_2_path='sec_edgar_filings/F/10-K/0000037996-19-000012.txt'\n",
    "\n",
    "soup=load_html(MSFT_1_path)\n",
    "\"\"\"\n",
    "#soup_get_text(soup)\n",
    "#visa, msft, goog, blk work\n",
    "#, ba_1_path uses <a id=\"links\">\n",
    "#ford dont work cuz href is at the page number in toc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#get table of content\n",
    "\n",
    "\"identify the mention of Signatures\"\n",
    "tag = soup.find('a',string=re.compile(\"Signatures\"))\n",
    "#print(tag)\n",
    "links = list()\n",
    "for elements in tag.previous_elements:\n",
    "    if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "        #print(elements)\n",
    "        links.append(elements.get('href')[1:])\n",
    "        \n",
    "links.reverse()\n",
    "#print_list(links)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the html\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(open(\"/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-19-027952.txt\"), 'html.parser')\n",
    "#print(soup.prettify())\n",
    "print(soup.get_text())\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#get all href from the table of content \n",
    "Links = list()\n",
    "for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "    if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "        Links.append(link.get('href')[1:])\n",
    "for link in Links:\n",
    "    print(link)\n",
    "        \n",
    "    #remove the '#' and store them in a list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get section \n",
    "#example get section between Item 1 and Item 1A\n",
    "\n",
    "#Problem also get the sentences\n",
    "\"\"\"\n",
    "Sections=\"\"\n",
    "for link in soup.find_all('a'):\n",
    "    if link.has_attr('name') and link['name']=='ITEM_1_BUSINESS':\n",
    "        for elements in link.next_elements:\n",
    "            if elements.name=='a' and elements.has_attr('name') and elements['name']=='ITEM_1B_UNRESOLVED_STAFF_COMMENTS':\n",
    "                break\n",
    "                \n",
    "            \n",
    "            if str(elements)[0] != '<':\n",
    "                Sections = Sections + str(elements)\n",
    "                print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            \n",
    "        break\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
