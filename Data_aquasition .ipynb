{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re\n",
    "import os.path\n",
    "my_path = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be implemented\n",
    "def check_parsing_error(file):\n",
    "    for section in sections:\n",
    "        if not(test_str2 and test_str2.strip()):\n",
    "            print(\"section is empty\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning approach:\n",
    "\n",
    "#First identify the table of content by regex to get mention of \"Item\" in <Table> <tbody> tag(perhaps extract the table of content)\n",
    "#or search <a href=\"#.....\" util the mention of Item16, signautre\n",
    "#extract all the href in <a> attribute in the table of content (ex. href1, href2, href3 …)\n",
    "\n",
    "#Second have algorithm to identify <a name=\"href1\"> and save the content between href1 and href2, href2 and href3, …\n",
    "\n",
    "#Third, clean the html tags in the contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html(path):\n",
    "    return BeautifulSoup(open(path,\"r\"), 'html.parser')\n",
    "def load_xml(path):\n",
    "    return BeautifulSoup(open(path,\"r\"), 'lxml')\n",
    "\n",
    "#get the text from soup\n",
    "def soup_get_text(soup):\n",
    "    print(soup.get_text())\n",
    "\n",
    "#write \"string\" to file from \"path\"\n",
    "def write_file(path, string):\n",
    "    text_file = open(path, \"w\")\n",
    "    text_file.write(string)\n",
    "    text_file.close()\n",
    "\n",
    "\n",
    "#print elements from list\n",
    "def print_list(lists):\n",
    "    for list in lists:\n",
    "        print(list)\n",
    "\n",
    "#read all the text file in the directory\n",
    "def file_reader(path):\n",
    "    files = []\n",
    "    for r,d,f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r,file))\n",
    "                \n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_table_of_content_links2(soup):\\n    links = list()\\n    for link in soup.find_all('a'):\\n    #link.get('href')\\n    #print(link.get('href'))\\n    #print(link.get('name'))\\n        if link.get('href')!=None and link.get('href')[0]=='#':\\n            links.append(link.get('href')[1:])\\n    return links\\n# or identify the signature link and use find_previous method to extract:extract()?\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First extract the location of the table of content by\n",
    "#using regex to identify \"Item 15\" or \"Financial Statement Schedules\"\n",
    "#get every links <a href=\"#\"> above \"Item 15\" or \"Financial Statement Schedules\"\n",
    "#only works with the text and items in table of content contains href#\n",
    "#if only in the page number, doesn't work\n",
    "def get_table_of_content_links(soup):\n",
    "    tag = soup.find('a',string=re.compile(\"Financial Statement Schedules\"))\n",
    "    tag_item = soup.find('a',string=re.compile(\"Item 15\"))\n",
    "    if tag==None and tag_item!=None:\n",
    "        tag = tag_item\n",
    "    \n",
    "    links = list()\n",
    "    links.append(tag.get('href')[1:])\n",
    "    for elements in tag.previous_elements:\n",
    "        if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "            #print(elements)\n",
    "            links.append(elements.get('href')[1:])\n",
    "    \n",
    "    links.reverse()\n",
    "    print(\"total section: \",len(remove_duplicates_in_list(links)))\n",
    "    return remove_duplicates_in_list(links)\n",
    "\n",
    "#remove dupicated items\n",
    "def remove_duplicates_in_list(List):\n",
    "    return list(dict.fromkeys(List))\n",
    "\"\"\"\n",
    "def get_table_of_content_links2(soup):\n",
    "    links = list()\n",
    "    for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "        if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "            links.append(link.get('href')[1:])\n",
    "    return links\n",
    "# or identify the signature link and use find_previous method to extract:extract()?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get every sections by using the links to identify <a>\n",
    "def get_all_sections(links, soup):\n",
    "    sections=list()\n",
    "    for i in range(len(links)):\n",
    "        if i<len(links)-1:\n",
    "            sections.append(get_section(links[i],links[i+1],soup))\n",
    "        else:\n",
    "            sections.append(get_last_section(links[i],soup))\n",
    "    return sections\n",
    "\n",
    "#get sections between links[i] and links[i+1]\n",
    "def get_section(link1, link2, soup):\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if elements.name=='a' and elements.has_attr('name') and elements['name']==link2:\n",
    "                    break \n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "        elif link.has_attr('id') and link['id']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if elements.name=='a' and elements.has_attr('id') and elements['id']==link2:\n",
    "                    break \n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str\n",
    "\n",
    "#get last section that is between <a name=\"links[n]\"> to the end of the document\n",
    "def get_last_section(link1,soup):\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "        elif link.has_attr('id') and link['id']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + \"\\n\" +str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the tag that is not <type>10-k using xml\n",
    "def clean_10k_by_xml(path):\n",
    "    soup=load_xml(path)\n",
    "    extract_type_10k(soup)\n",
    "    write_file(path, str(soup))\n",
    "\n",
    "#remove all tag that is not <type>10-k\n",
    "def extract_type_10k(soup):\n",
    "    tag_list=soup.find_all('type')\n",
    "    for tag in tag_list:\n",
    "        print(tag.next_element)\n",
    "        if '10-K' not in tag.next_element:\n",
    "            tag.extract()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First remove all the tag that is not <type>10-k including xml\n",
    "#Secont extract the links of the items from the table of content \n",
    "#Third get all the sections and store it in a list\n",
    "def get_10k_sections(path):\n",
    "    soup=load_html(path)\n",
    "    sections=get_all_sections(get_table_of_content_links(load_html(path)),soup)\n",
    "    return sections\n",
    "\n",
    "#get all the path of 10k file and store in list name files\n",
    "def get_10k_files():\n",
    "    files = [];\n",
    "    files = file_reader(my_path+\"/sec_edgar_filings\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sections[0])\n",
    "def save_sections(tenk_filings):\n",
    "    report_number = 0\n",
    "    section_num =0\n",
    "    for report in tenk_filings:\n",
    "        if not os.path.exists(my_path+'/clean_data/filing_'+str(report_number)):\n",
    "            os.mkdir(my_path+'/clean_data/filing_'+str(report_number))\n",
    "            print(\"Directory \" , '/clean_data/filing_'+str(report_number) ,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , '/clean_data/filing_'+str(report_number) ,  \" already exists\")\n",
    "        \n",
    "        for section in report:\n",
    "            write_file('clean_data/filing_'+str(report_number)+'/filing_'+str(report_number)+'_section_'+str(section_num)+\".txt\",section)\n",
    "            section_num = section_num+1\n",
    "        report_number = report_number+1\n",
    "\n",
    "def save_sections(filing, path, report_number):\n",
    "    section_num =0\n",
    "    for section in filing:\n",
    "        write_file(path+'/filing_'+str(report_number)+'_section_'+str(section_num)+\".txt\",section)\n",
    "        section_num = section_num+1\n",
    "\n",
    "\n",
    "def save_filings_by_section(filings, path):\n",
    "    report_number = 0\n",
    "    if not os.path.exists(path+'/clean_data'):\n",
    "        os.mkdir(path+'/clean_data')\n",
    "        print(\"Directory \" , '/clean_data/',  \" Created \")\n",
    "    for filing in tenk_filings:\n",
    "        if not os.path.exists(path+'/clean_data/filing_'+str(report_number)):\n",
    "            os.mkdir(path+'/clean_data/filing_'+str(report_number))\n",
    "            print(\"Directory \" , '/clean_data/filing_'+str(report_number) ,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , '/clean_data/filing_'+str(report_number) ,  \" already exists\")\n",
    "        \n",
    "        \n",
    "        save_sections(filing,path+'/clean_data/filing_'+str(report_number), report_number)\n",
    "        report_number = report_number+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0000320193-19-000119.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0001628280-16-020309.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0000320193-17-000070.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0001193125-15-356351.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0000320193-18-000145.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-19-000004.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-17-000008.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-20-000008.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-16-000012.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-18-000007.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-19-005479.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-17-002816.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-20-007807.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-18-003744.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-16-013511.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-18-019062.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-17-014900.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001193125-15-272806.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001193125-16-662209.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-19-027952.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-19-000010.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-16-000099.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-18-000007.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-17-000006.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-20-000014.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-17-000044.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-16-000058.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-18-000055.txt\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-19-000050.txt\n"
     ]
    }
   ],
   "source": [
    "files_path=list()\n",
    "files_path=get_10k_files()\n",
    "print_list(files_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0000320193-19-000119.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0001628280-16-020309.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0000320193-17-000070.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0001193125-15-356351.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/AAPL/10-K/0000320193-18-000145.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-19-000004.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-17-000008.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-20-000008.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-16-000012.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/GOOG/10-K/0001652044-18-000007.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-19-005479.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-17-002816.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-20-007807.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-18-003744.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BLK/10-K/0001564590-16-013511.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-18-019062.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-17-014900.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001193125-15-272806.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001193125-16-662209.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-19-027952.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-19-000010.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-16-000099.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-18-000007.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-17-000006.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/BA/10-K/0000012927-20-000014.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-17-000044.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-16-000058.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-18-000055.txt\n",
      "10-K\n",
      "\n",
      "/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/V/10-K/0001403161-19-000050.txt\n",
      "10-K\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files_path:\n",
    "    print(file)\n",
    "    clean_10k_by_xml(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total section:  24\n",
      "total section:  24\n",
      "total section:  24\n",
      "total section:  25\n",
      "total section:  24\n",
      "total section:  22\n",
      "total section:  22\n",
      "total section:  22\n",
      "total section:  22\n",
      "total section:  23\n",
      "total section:  20\n",
      "total section:  20\n",
      "total section:  20\n",
      "total section:  20\n",
      "total section:  20\n",
      "total section:  23\n",
      "total section:  23\n",
      "total section:  24\n",
      "total section:  24\n",
      "total section:  23\n",
      "total section:  25\n",
      "total section:  24\n",
      "total section:  25\n",
      "total section:  24\n",
      "total section:  25\n",
      "total section:  25\n",
      "total section:  25\n",
      "total section:  25\n",
      "total section:  25\n"
     ]
    }
   ],
   "source": [
    "sections=list()\n",
    "tenk_filings = list()\n",
    "for file in files_path:\n",
    "    sections=get_10k_sections(file)\n",
    "    tenk_filings.append(sections)\n",
    "\n",
    "#save_filings_by_section(tenk_filings, my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /clean_data/filing_0  already exists\n",
      "Directory  /clean_data/filing_1  Created \n",
      "Directory  /clean_data/filing_2  Created \n",
      "Directory  /clean_data/filing_3  Created \n",
      "Directory  /clean_data/filing_4  Created \n",
      "Directory  /clean_data/filing_5  Created \n",
      "Directory  /clean_data/filing_6  Created \n",
      "Directory  /clean_data/filing_7  Created \n",
      "Directory  /clean_data/filing_8  Created \n",
      "Directory  /clean_data/filing_9  Created \n",
      "Directory  /clean_data/filing_10  Created \n",
      "Directory  /clean_data/filing_11  Created \n",
      "Directory  /clean_data/filing_12  Created \n",
      "Directory  /clean_data/filing_13  Created \n",
      "Directory  /clean_data/filing_14  Created \n",
      "Directory  /clean_data/filing_15  Created \n",
      "Directory  /clean_data/filing_16  Created \n",
      "Directory  /clean_data/filing_17  Created \n",
      "Directory  /clean_data/filing_18  Created \n",
      "Directory  /clean_data/filing_19  Created \n",
      "Directory  /clean_data/filing_20  Created \n",
      "Directory  /clean_data/filing_21  Created \n",
      "Directory  /clean_data/filing_22  Created \n",
      "Directory  /clean_data/filing_23  Created \n",
      "Directory  /clean_data/filing_24  Created \n",
      "Directory  /clean_data/filing_25  Created \n",
      "Directory  /clean_data/filing_26  Created \n",
      "Directory  /clean_data/filing_27  Created \n",
      "Directory  /clean_data/filing_28  Created \n"
     ]
    }
   ],
   "source": [
    "save_filings_by_section(tenk_filings, my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLK file 20 link in <p id=\" \"\n",
    "#visa file 2015 have link from item 1-14 but probably forgot to add link to < a item 15 or < a Financial Statement Schedules\n",
    "#visa file 2015 add link to the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext_file = open(\"sample_KG.txt\", \"w\")\\ntext_file.write(sections[2])\\ntext_file.close()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove all xml tag and save it \n",
    "\"\"\"\n",
    "remove_xml(soup1)\n",
    "text_file = open(\"sec_edgar_filings/clean_data/sample_10k.txt\", \"w\")\n",
    "text_file.write(str(soup1))\n",
    "text_file.close()\n",
    "\"\"\"\n",
    "#load html soup and prettify\n",
    "\"\"\"\n",
    "soup=load_html(\"sec_edgar_filings/clean_data/sample_10k.txt\")\n",
    "soup.prettify()\n",
    "\"\"\"\n",
    "#get table of content links\n",
    "\"\"\"\n",
    "links = get_table_of_content_links1(soup)\n",
    "print_list(links)\n",
    "print(len(links))\n",
    "\"\"\"\n",
    "#get all sections in 10k\n",
    "\"\"\"\n",
    "sections=list()\n",
    "get_all_sections(sections,links)\n",
    "print(sections[21])\n",
    "\"\"\"\n",
    "#write sections to a txt file\n",
    "\"\"\"\n",
    "text_file = open(\"sample_KG.txt\", \"w\")\n",
    "text_file.write(sections[2])\n",
    "text_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMSFT_1_path=\\'sec_edgar_filings/MSFT/10-K/0001193125-15-272806.txt\\'\\nMSFT_2_path=\"sec_edgar_filings/MSFT/10-K/0001564590-18-019062.txt\"\\nAPPL_1_path=\\'sec_edgar_filings/AAPL/10-K/0000320193-19-000119.txt\\'\\nAPPL_2_path=\\'sec_edgar_filings/AAPL/10-K/0000320193-18-000145.txt\\'\\nVISA_1_path=\\'sec_edgar_filings/V/10-K/0001403161-18-000055.txt\\'\\nVISA_2_path=\\'sec_edgar_filings/V/10-K/0001403161-19-000050.txt\\'\\nGOOG_1_path=\\'sec_edgar_filings/GOOG/10-K/0001652044-19-000004.txt\\'\\nGOOG_2_path=\\'sec_edgar_filings/GOOG/10-K/0001652044-20-000008.txt\\'\\nBLK_1_path=\\'sec_edgar_filings/BLK/10-K/0001564590-19-005479.txt\\'\\nBLK_2_path=\\'sec_edgar_filings/BLK/10-K/0001564590-20-007807.txt\\'\\nBA_1_path=\\'sec_edgar_filings/BA/10-K/0000012927-19-000010.txt\\'\\nBA_2_path=\\'sec_edgar_filings/BA/10-K/0000012927-20-000014.txt\\'\\nFORD_1_path=\\'sec_edgar_filings/F/10-K/0000037996-20-000010.txt\\'\\nFORD_2_path=\\'sec_edgar_filings/F/10-K/0000037996-19-000012.txt\\'\\n\\nsoup=load_html(MSFT_1_path)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MSFT_1_path='sec_edgar_filings/MSFT/10-K/0001193125-15-272806.txt'\n",
    "MSFT_2_path=\"sec_edgar_filings/MSFT/10-K/0001564590-18-019062.txt\"\n",
    "APPL_1_path='sec_edgar_filings/AAPL/10-K/0000320193-19-000119.txt'\n",
    "APPL_2_path='sec_edgar_filings/AAPL/10-K/0000320193-18-000145.txt'\n",
    "VISA_1_path='sec_edgar_filings/V/10-K/0001403161-18-000055.txt'\n",
    "VISA_2_path='sec_edgar_filings/V/10-K/0001403161-19-000050.txt'\n",
    "GOOG_1_path='sec_edgar_filings/GOOG/10-K/0001652044-19-000004.txt'\n",
    "GOOG_2_path='sec_edgar_filings/GOOG/10-K/0001652044-20-000008.txt'\n",
    "BLK_1_path='sec_edgar_filings/BLK/10-K/0001564590-19-005479.txt'\n",
    "BLK_2_path='sec_edgar_filings/BLK/10-K/0001564590-20-007807.txt'\n",
    "BA_1_path='sec_edgar_filings/BA/10-K/0000012927-19-000010.txt'\n",
    "BA_2_path='sec_edgar_filings/BA/10-K/0000012927-20-000014.txt'\n",
    "FORD_1_path='sec_edgar_filings/F/10-K/0000037996-20-000010.txt'\n",
    "FORD_2_path='sec_edgar_filings/F/10-K/0000037996-19-000012.txt'\n",
    "\n",
    "soup=load_html(MSFT_1_path)\n",
    "\"\"\"\n",
    "#soup_get_text(soup)\n",
    "#visa, msft, goog, blk work\n",
    "#, ba_1_path uses <a id=\"links\">\n",
    "#ford dont work cuz href is at the page number in toc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#get table of content\\n\\n\"identify the mention of Signatures\"\\ntag = soup.find(\\'a\\',string=re.compile(\"Signatures\"))\\n#print(tag)\\nlinks = list()\\nfor elements in tag.previous_elements:\\n    if elements.name==\\'a\\' and elements.has_attr(\\'href\\') and elements.get(\\'href\\')[0]==\\'#\\':\\n        #print(elements)\\n        links.append(elements.get(\\'href\\')[1:])\\n        \\nlinks.reverse()\\n#print_list(links)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#get table of content\n",
    "\n",
    "\"identify the mention of Signatures\"\n",
    "tag = soup.find('a',string=re.compile(\"Signatures\"))\n",
    "#print(tag)\n",
    "links = list()\n",
    "for elements in tag.previous_elements:\n",
    "    if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "        #print(elements)\n",
    "        links.append(elements.get('href')[1:])\n",
    "        \n",
    "links.reverse()\n",
    "#print_list(links)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsoup = BeautifulSoup(open(\"/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-19-027952.txt\"), \\'html.parser\\')\\n#print(soup.prettify())\\nprint(soup.get_text())\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the html\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(open(\"/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-19-027952.txt\"), 'html.parser')\n",
    "#print(soup.prettify())\n",
    "print(soup.get_text())\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#get all href from the table of content \\nLinks = list()\\nfor link in soup.find_all('a'):\\n    #link.get('href')\\n    #print(link.get('href'))\\n    #print(link.get('name'))\\n    if link.get('href')!=None and link.get('href')[0]=='#':\\n        Links.append(link.get('href')[1:])\\nfor link in Links:\\n    print(link)\\n        \\n    #remove the '#' and store them in a list\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#get all href from the table of content \n",
    "Links = list()\n",
    "for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "    if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "        Links.append(link.get('href')[1:])\n",
    "for link in Links:\n",
    "    print(link)\n",
    "        \n",
    "    #remove the '#' and store them in a list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSections=\"\"\\nfor link in soup.find_all(\\'a\\'):\\n    if link.has_attr(\\'name\\') and link[\\'name\\']==\\'ITEM_1_BUSINESS\\':\\n        for elements in link.next_elements:\\n            if elements.name==\\'a\\' and elements.has_attr(\\'name\\') and elements[\\'name\\']==\\'ITEM_1B_UNRESOLVED_STAFF_COMMENTS\\':\\n                break\\n                \\n            \\n            if str(elements)[0] != \\'<\\':\\n                Sections = Sections + str(elements)\\n                print(repr(elements))\\n            #Sections = Sections + str(elements)\\n            \\n        break\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get section \n",
    "#example get section between Item 1 and Item 1A\n",
    "\n",
    "#Problem also get the sentences\n",
    "\"\"\"\n",
    "Sections=\"\"\n",
    "for link in soup.find_all('a'):\n",
    "    if link.has_attr('name') and link['name']=='ITEM_1_BUSINESS':\n",
    "        for elements in link.next_elements:\n",
    "            if elements.name=='a' and elements.has_attr('name') and elements['name']=='ITEM_1B_UNRESOLVED_STAFF_COMMENTS':\n",
    "                break\n",
    "                \n",
    "            \n",
    "            if str(elements)[0] != '<':\n",
    "                Sections = Sections + str(elements)\n",
    "                print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            \n",
    "        break\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
