{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning approach:\n",
    "\n",
    "#First identify the table of content by regex to get mention of \"Item\" in <Table> <tbody> tag(perhaps extract the table of content)\n",
    "#or search <a href=\"#.....\" util the mention of Item16, signautre\n",
    "#extract all the href in <a> attribute in the table of content (ex. href1, href2, href3 …)\n",
    "\n",
    "#Second have algorithm to identify <a name=\"href1\"> and save the content between href1 and href2, href2 and href3, …\n",
    "\n",
    "#Third, clean the html tags in the contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html(path):\n",
    "    return BeautifulSoup(open(path), 'html.parser')\n",
    "\n",
    "def soup_get_text(soup):\n",
    "    print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First extract the location of the table of content \n",
    "# use regex to identify Signature/Item 16 and get everything above\n",
    "#only works with the text and items in table of content contains href#if only in the page number, doesn't work\n",
    "\n",
    "def remove_duplicates_in_list(List):\n",
    "    return list(dict.fromkeys(List))\n",
    "def get_table_of_content_links1(soup):\n",
    "    tag = soup.find('a',string=re.compile(\"Financial Statement Schedules\"))\n",
    "    tag_item = soup.find('a',string=re.compile(\"Item 15\"))\n",
    "    if tag==None and tag_item!=None:\n",
    "        tag = tag_item\n",
    "    \n",
    "    links = list()\n",
    "    links.append(tag.get('href')[1:])\n",
    "    for elements in tag.previous_elements:\n",
    "        if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "            #print(elements)\n",
    "            links.append(elements.get('href')[1:])\n",
    "    \n",
    "    links.reverse()\n",
    "    return remove_duplicates_in_list(links)\n",
    "    #print_list(links)\n",
    "\n",
    "#\n",
    "def get_table_of_content_links2(soup):\n",
    "    links = list()\n",
    "    for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "        if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "            links.append(link.get('href')[1:])\n",
    "    return links\n",
    "# or identify the signature link and use find_previous method to extract:extract()?\n",
    "\n",
    "def print_list(lists):\n",
    "    for list in lists:\n",
    "        print(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unable to format tables\n",
    "def get_all_sections(sections, links):\n",
    "    for i in range(len(links)):\n",
    "        if i<len(links)-1:\n",
    "            sections.append(get_section(links[i],links[i+1]))\n",
    "        else:\n",
    "            sections.append(get_last_section(links[i]))\n",
    "\n",
    "def get_section(link1, link2):\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if elements.name=='a' and elements.has_attr('name') and elements['name']==link2:\n",
    "                    break \n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str\n",
    "\n",
    "def get_last_section(link1):\n",
    "    section_str=\"\"\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('name') and link['name']==link1:\n",
    "            for elements in link.next_elements:\n",
    "                if str(elements)[0] != '<':\n",
    "                    section_str = section_str + str(elements)\n",
    "                    #print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            break\n",
    "    return section_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"\""
    "\n",
    "soup=load_html(path)\n",
    "#soup_get_text(soup)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"#sC8BE3318CBF05904A4195064A5A404FE\" style=\"font-family:Helvetica,sans-serif;font-size:9pt;\"><span style=\"font-family:Helvetica,sans-serif;font-size:9pt;\">Item 15.</span></a>\n",
      "sE45D95EC302C53F3999A16F238922716\n",
      "s8FA93AA667C0535398014FA54B397439\n",
      "s107EA6B564A4575DA42B4A73ACD9C71A\n",
      "s09BC0FC06350577DAA71D344EA07A929\n",
      "s91DF3ECA7CDE54B296324D6C925A3A64\n",
      "sE96BF62935E55A508C134F5B2F0CA85E\n",
      "sD33472E8BE865C43851A1C3B62FDB2D2\n",
      "s441770BCD3195825A459C905D90A255A\n",
      "sF8C8F387D3165D459768F3D870FAEA13\n",
      "sEDE9BAA1C9E05AB4843082ACD6A97FE3\n",
      "s86531024D4355D39AEEBA31406A0E758\n",
      "s37F84E3E4CEC530F9DE46181A4344271\n",
      "sDBCC0D7FC5D05F49A572F9AA0627E992\n",
      "s6B01DE738B9E59FF8C3F8CFE9C9E0B79\n",
      "sA9A243DA245A56F5AC25705E9A3D277F\n",
      "s05D204CD87755D688D28C56362110879\n",
      "s046E02B1765C5FC98134C77D8F55508A\n",
      "sF9CD356C29FA5DC18DCAC248942C78F9\n",
      "sD1B069C378005F3AB1606C1AADA06047\n",
      "s196AADBFDD535C31B8B9CD690EED64EC\n",
      "sC8A90ABF36F05F8CBA046AD164ED5378\n",
      "sFEE386E5673257028D26E7FE59FD1E21\n",
      "s49498D0CD5C959CA84C417CEA46A6953\n",
      "sC8BE3318CBF05904A4195064A5A404FE\n"
     ]
    }
   ],
   "source": [
    "links = get_table_of_content_links1(soup)\n",
    "print_list(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections=list()\n",
    "get_all_sections(sections,links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM 1B.UNRESOLVED STAFF COMMENTSNot applicable.\n"
     ]
    }
   ],
   "source": [
    "print(sections[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#get table of content\n",
    "\n",
    "\"identify the mention of Signatures\"\n",
    "tag = soup.find('a',string=re.compile(\"Signatures\"))\n",
    "#print(tag)\n",
    "links = list()\n",
    "for elements in tag.previous_elements:\n",
    "    if elements.name=='a' and elements.has_attr('href') and elements.get('href')[0]=='#':\n",
    "        #print(elements)\n",
    "        links.append(elements.get('href')[1:])\n",
    "        \n",
    "links.reverse()\n",
    "#print_list(links)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the html\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(open(\"/Users/davidren/Desktop/Education/KTH/2019-2020/2020 Spring/Master Thesis/Data/sec_edgar_filings/MSFT/10-K/0001564590-19-027952.txt\"), 'html.parser')\n",
    "#print(soup.prettify())\n",
    "print(soup.get_text())\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#get all href from the table of content \n",
    "Links = list()\n",
    "for link in soup.find_all('a'):\n",
    "    #link.get('href')\n",
    "    #print(link.get('href'))\n",
    "    #print(link.get('name'))\n",
    "    if link.get('href')!=None and link.get('href')[0]=='#':\n",
    "        Links.append(link.get('href')[1:])\n",
    "for link in Links:\n",
    "    print(link)\n",
    "        \n",
    "    #remove the '#' and store them in a list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get section \n",
    "#example get section between Item 1 and Item 1A\n",
    "\n",
    "#Problem also get the sentences\n",
    "\"\"\"\n",
    "Sections=\"\"\n",
    "for link in soup.find_all('a'):\n",
    "    if link.has_attr('name') and link['name']=='ITEM_1_BUSINESS':\n",
    "        for elements in link.next_elements:\n",
    "            if elements.name=='a' and elements.has_attr('name') and elements['name']=='ITEM_1B_UNRESOLVED_STAFF_COMMENTS':\n",
    "                break\n",
    "                \n",
    "            \n",
    "            if str(elements)[0] != '<':\n",
    "                Sections = Sections + str(elements)\n",
    "                print(repr(elements))\n",
    "            #Sections = Sections + str(elements)\n",
    "            \n",
    "        break\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
